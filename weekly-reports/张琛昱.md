# 大四春季学期毕设周报

有关毕设的相关文档、资料，可在这里找到→ [Graduation Project](https://github.com/Pacific73/GraduationProject)

## 第四周 2018.3.19 - 2018.3.23

### 主要进展

- 周末在新机器上进行实验，结果很奇怪，cache缺失率非常低。旧机子上缺失率却很高。
- 周一测试了一个简单的benchmark，发现在有干扰的情况下，latency增加得并不明显。
  - 怀疑是测试程序有问题。又测试了顺序访问，调整了步距。延迟上升更多。
  - 怀疑是干扰核数不够多。开多了干扰核数。开多核数其实是科学的，因为在实际的生产环境中，Batch Effort任务一般也是多进（线）程的。
- 开始考虑多线程干扰，而非多进程。原因在于多进程干扰无法确定实际被干扰的cache块大小。
- 进行了多次cache干扰实验。实验结果在[这里](https://github.com/Pacific73/GraduationProject/blob/master/notes/cache_experiment.md)。
  - 随机访问干扰的效果没有stream方式明显。
  - 实验报告中给出了比较合理的解释，可以算作比较的实验结果。
- 在确定怀疑的第二个原因：memory bandwidth saturation时，发现缺少相应的监测工具。Intel提供了一款工具[PCM](https://github.com/opcm/pcm)，但经过试验，只支持Intel Xeon E5/E7系列。由于不影响试验测试，暂且搁置一旁。（在进行Heracles重现时，可能会用到）
- 阅读论文[OMBM: Optimized memory bandwidth management for ensuring QoS and high server utilization](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8064134)。该论文从OS层面，为进行memory bandwidth控制提供了一种基于sample+predict+restriction的软件机制。可以之后复现Heracles时借鉴。

### 下周计划

- 周末进行memory干扰测试。只需把cache干扰块取大即可，同Heracles一文中一样。
- 进行Hyperthread、memory干扰测试。争取下周能有好的测试结果，迎接中期答辩。

## 第三周 2018.3.11 - 2018.3.16

### 主要进展

- 大致阅读了xapian (websearch)的测试源码，了解了测试原理。
- 发现了client程序也会在LLC上对server产生干扰，有必要搭建network环境。
- 搭建好network测试环境，并针对几个局部点进行了简单的测试。
  - 测试效果并没有Heracles论文中那样出现>300%的latency波动，而是缓慢地增长。
  - 而且利用perf工具，确实发现cache缺失率从4%骤增至40+%，却没有带来显著的latency增加。
  - 猜想
    - Heracles一文中针对latency增加提出了两种假设：
      1. BE load导致的working instruction set missing
         但是实验环境包含L1i和L1d cache，可能不会导致working instruction set missing. (L1i cache 大小为32KB，可以装下一定量的程序)
      2. cache大量缺失导致的DRAM带宽饱和
         在实验环境下，可能DRAM饱和并不是那么容易，因此并没有带来明显的延迟。
    - 局部点不足以说明全局情况。
      - 增加实验测量点。
    - 可能与测试程序特性有关。
      - 更换其他测试程序，如masstree (张乾宇正在研究)。
- 准备阿里实习面试。
- 学习shell。

## 第二周 2018.3.5 - 2018.3.9

### 主要进展

- 整理毕设思路。见[整体思路](https://github.com/Pacific73/GraduationProject/blob/master/notes/%E6%95%B4%E4%BD%93%E6%80%9D%E8%B7%AF.md)。
- 阅读了[TailBench: A Benchmark Suite and Evaluation Methodology for Latency-Critical Applications](http://people.csail.mit.edu/sanchez/papers/2016.tailbench.iiswc.pdf)。
  - 一个用来测试LC任务latency的测试集，基于client-server的程序设计理念。
  - 有loopback/networked/integrated三种模式
    - loopback为本地多进程测量，包含本机网络协议栈延迟。
    - networked为多机测量，包含机器间网络协议栈延迟及网络传输延迟。
    - integrated为本机单进程测量，数据包通过内存直接通信，不经过网络协议栈。
  - 8个标准测试程序，涵盖websearch/key-value存储/在线ml任务响应等。
  - 配置并跑通了silo，xapian，masstree和moses四个测试程序。
    - guide: [install tailbench](https://github.com/riscv-labs/references/blob/master/Install_tailbench.md)
  - tailbench为干扰实验提供了最基本的测量指标。
- loopback模式下实验有如下注意事项：
  - 把cpu主频限制在一个稳定的值。
    - 利用cpupower(文档：[1](https://wiki.archlinux.org/index.php/CPU_frequency_scaling_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)),[2](https://www.kernel.org/doc/Documentation/cpu-freq/governors.txt))以cpufrequtils.
  - 将干扰程序与server放在一个核上，将client放在其他核上。
    - taskset
  - 注意利用cpulimit限制干扰程序的cpu占用率，防止占满CPU导致tailbench测试程序进行网络包发送时发生错误，从而导致崩溃的情形。
- 针对cache的干扰程序
  - 定间隔访问
  - 随机访问

### 存在的问题及解决方案

- 在小盒子上进行测量时发现无法锁定CPU frequency，使用了张乾宇提供的方法后仍达不到指定效果。
  - 最后更换到杨兴杲处的服务器上后，采用相同策略，问题得到了解决。
  - 猜测问题是出在小盒子的功率管理功能上。
- 在xapian(websearch)上进行了实验。实验效果较为理想。
  - 干扰前，限制了CPU频率等条件后，测量结果非常稳定。
  - 加入干扰后，latency的上升非常明显（在相同logic core上混部）。

### 下周计划

- cache的干扰数据可以进行测量了。全部拿到baseline和数据。
- 查看memory的干扰方法。



## 第一周 2018.2.26 - 2018.3.2

请假。