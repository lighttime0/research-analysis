# TVM/NNVM 研究进展报告

## 当前背景

这个问题是我在上海参加SOSP的过程中发现的。去上海参加SOSP前，我定好了参会的目标，就是去找seda的方法可以用在当前哪些研究热点上。

在参加AI Workshop的过程中，发现很多演讲者都对语言和框架的支持有很大需求，包括最后一个环节，四位业界领头人对未来研究趋势的探讨中，提到了python语言、AI框架。会后，我给提到这些问题的一位专家发了邮件，问了一些具体的问题，得到回复大致内容是希望在语言层能有更好的支持，比如在编译层的加速，或者在AST上的将操作合并，并提到可以参考TVM/NNVM。

回来后了解了一下TVM/NNVM。它是一个框架，一个可以复用很多操作的框架，用于快速搭建机器学习框架。其设计思路来源于LLVM，正如LLVM可以很快速方便的设计自己的编译器和优化器，NNVM可以很快速方便的设计自己的机器学习框架。[TinyFlow](https://github.com/tqchen/tinyflow)是一个样例，基于NNVM，用2k行的代码大致实现了TensorFlow的API。

可以理解为，机器学习的框架很大一部分功能在于提供运算的API，比如卷积等等。而这些API很大程度上可以复用，NNVM提供了这样一种复用。NNVM和LLVM在概念、代码上都比较类似。

另外，关于加速，python由于动态类型等原因，对优化的支持不好，上面提到过一种折中的思路是在AST上进行一些操作的合并来加速。这就是我对它进行调研的原因。

## 研究现状

这一块是一个比较新的东西，NNVM是16年底才出现的东西（NNVM是语言层的框架，TVM是NNVM的底层支持），还没有论文发出来。Amazon已经将TVM作为MXnet的后端了。

## 挑战

目前还是处于调研阶段，上面那个在AST上进行操作的合并来进行加速也只是一个看起来能做的问题。我能不能做还要跟陈老师去讨论。

对TVM/NNVM目前处于调研阶段，在清华只有类脑计算中心的吴臻志老师2018年有一篇博客提到TVM/NNVM。

